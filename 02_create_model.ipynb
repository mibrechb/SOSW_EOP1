{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba1e38-b579-4750-8dfa-b9503c475539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shapely\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import constants as c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d006b5-352b-4f21-bee9-76190adde51e",
   "metadata": {},
   "source": [
    "## 1. Setup GEE API and Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d03778-c932-4320-83e5-a97cb7e7347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae762a17-1925-4b60-961f-3fee6c7b8963",
   "metadata": {},
   "source": [
    "## 2. Get MRC Metadata and plot stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f80ff4-05a6-4725-b7ee-71e5e891b6c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import urllib.request\n",
    "\n",
    "# def get_mrc_metadata(return_gdf=True, verbose=False):\n",
    "#     \"\"\" Get Metadata from Mekong River Comission Data Portal. \"\"\"\n",
    "#     url = r'https://api.mrcmekong.org/api/v1/ts/inventory/timeSeriesList'\n",
    "#     urllib.request.urlretrieve(url, 'timeSeriesList.json')\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(f'Downloaded time-series metadata from {url} .')\n",
    "    \n",
    "#     f = open('timeSeriesList.json')\n",
    "#     data = json.load(f)\n",
    "#     df_metadata = pd.DataFrame([])\n",
    "#     for dataset in data:\n",
    "#         df_temp = pd.DataFrame([dict(dataset)])\n",
    "#         df_temp['longitude'] = df_temp['longitude'].astype(float)\n",
    "#         df_temp['latitude'] = df_temp['latitude'].astype(float)\n",
    "#         df_metadata = pd.concat([df_temp, df_metadata])\n",
    "#         f.close()\n",
    "#     df_metadata = df_metadata.reset_index().drop(columns=['index'])\n",
    "    \n",
    "#     if verbose:\n",
    "#         print(f'Found a total of {df_metadata.shape[0]} time-series datasets from {len(df_metadata.stationCode.unique())} stations of the MRC Data Portal.')\n",
    "        \n",
    "#     if return_gdf:\n",
    "#         gdf_metadata = gpd.GeoDataFrame(\n",
    "#             df_metadata, geometry=gpd.points_from_xy(df_metadata.longitude, df_metadata.latitude), crs=\"EPSG:4326\"\n",
    "#         )\n",
    "#         return(gdf_metadata)\n",
    "#     else:\n",
    "#         return(df_metadata)\n",
    "\n",
    "# Map = geemap.Map(center=(40, -100), zoom=4)\n",
    "# Map\n",
    "\n",
    "# gdf_metadata = get_mrc_metadata(return_gdf=True)\n",
    "# gdf_metadata_sedi = gdf_metadata.loc[gdf_metadata.parameter=='Sediment Concentration']\n",
    "\n",
    "# # Get DSMP station metadata\n",
    "# gdf_metadata_dmsp = gdf_metadata_sedi.loc[gdf_metadata_sedi.label.str.contains('DSMP')]\n",
    "# gdf_stations_dsmp = gdf_metadata_dmsp.groupby('locationIdentifier').first()[['river', 'stationShortName', 'geometry']].set_crs('EPSG:4326')\n",
    "\n",
    "# # Get Hydromet stations metadata\n",
    "# gdf_metadata_hydromet = gdf_metadata_sedi.loc[~gdf_metadata_sedi.label.str.contains('DSMP')]\n",
    "# gdf_stations_hydromet = gdf_metadata_hydromet.groupby('locationIdentifier').first()[['river', 'stationShortName', 'geometry']].set_crs('EPSG:4326')\n",
    "\n",
    "# # # Add stations\n",
    "# Map.add_gdf(gdf_stations_dsmp, 'MRC DSMP stations', style={'fillColor': 'blue'})\n",
    "# Map.add_gdf(gdf_stations_hydromet, 'MRC Hydromet stations', style={'fillColor': 'red'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89feef-3b44-43e0-a536-41a500bcedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load 3S basin\n",
    "# json_data = 'geometries/geoms.geojson'\n",
    "# fc_geoms = geemap.geojson_to_ee(json_data)\n",
    "# roi_geom = fc_geoms.first().geometry()\n",
    "\n",
    "# # load dams\n",
    "# df = pd.read_csv('geometries/3SReservoirs.csv')\n",
    "# gdf_dams = gpd.GeoDataFrame(df, geometry=gpd.GeoSeries.from_xy(df['X'], df['Y']), crs=4326).drop(columns=['X', 'Y']).set_index('id')\n",
    "# fc_dams = geemap.geopandas_to_ee(gdf_dams)\n",
    "# Map.add_gdf(gdf_dams, 'Dams', {'color': 'blue'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d34ab3-e735-4033-99a0-02fee254af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data from local .csv files (see 01_insitu_preparation.ipynb)\n",
    "# paths_data_s = list(Path(f'../mrc_webscrapper/outputs/csv/Sediment Concentration/').glob(f'*.csv'))\n",
    "# paths_data_q = list(Path(f'../mrc_webscrapper/outputs/csv/Discharge/').glob(f'*.csv'))\n",
    "# paths_data = paths_data_q + paths_data_s\n",
    "# df_data = pd.DataFrame([])\n",
    "# for path in paths_data_s:\n",
    "#     df_temp = pd.read_csv(path, dtype={'station_code':'str'})\n",
    "#     df_temp['date_utc'] = pd.to_datetime(df_temp['date'])\n",
    "#     df_temp['med_frq'] = np.median(np.diff(df_temp.date_utc))\n",
    "#     df_data = pd.concat([df_data, df_temp])\n",
    "\n",
    "# df_stations = df_data.groupby('station_code').first()\n",
    "# gdf_stations = gpd.GeoDataFrame(df_stations,\n",
    "#                  crs={'init': 'epsg:4326'},\n",
    "#                  geometry=df_stations.apply(lambda row: shapely.geometry.Point((row.lon, row.lat)), axis=1)\n",
    "#                 )\n",
    "\n",
    "# # Create geolocated DSMP/Hydromet datasets\n",
    "# df_data_dsmp = df_data.loc[df_data.identifier.str.contains('DSMP')]\n",
    "# gdf_data_dsmp = gpd.GeoDataFrame(df_data_dsmp.join(gdf_stations.geometry, on='station_code'))\n",
    "# df_data_hydromet = df_data.loc[~df_data.identifier.str.contains('DSMP')]\n",
    "# gdf_data_hydromet = gpd.GeoDataFrame(df_data_hydromet.join(gdf_stations.geometry, on='station_code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24610c-8a86-4111-beac-b1ce01853ddb",
   "metadata": {},
   "source": [
    "## 3. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e353d-db14-492f-bee1-2e432be76d47",
   "metadata": {},
   "source": [
    "### Define settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb27e5-673a-420d-bb03-db327072b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = '2000-01-01', '2024-07-05' # timespan to extract\n",
    "cld_filt_thresh = 70 # scene-based cloud filter\n",
    "cld_buffer, water_buffer = 250, 30 # cloud (grow) and water (erosion) buffer (meters)\n",
    "watermask = 'index' # type of watermask to apply\n",
    "sampling_buffer = 100 # sampling buffer (meters) and reducer\n",
    "gdrive_folder = r'E:\\Google Drive\\Earth Engine'\n",
    "output_folder = 'SOSW_SPM_SR_14052024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import functions_process as funcs_process\n",
    "import functions_turbidity as funcs_turb\n",
    "import functions_sampling as funcs_sampling\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# prepare insitu data, remove WQMN (surface grab samples)\n",
    "df = gpd.read_file('input/insitu_data.csv', ignore_geometry=True)\n",
    "df['geometry'] = gpd.GeoSeries.from_wkt(df['geometry']).set_crs('4326')\n",
    "gdf_data = gpd.GeoDataFrame(df)\n",
    "gdf_data = gdf_data.loc[~(gdf_data.geometry.is_empty)]\n",
    "gdf_data = gdf_data.loc[~(gdf_data.source=='WQMN')]\n",
    "\n",
    "settings = {\n",
    "    'start_date':       start_date,\n",
    "    'end_date':         end_date,\n",
    "    'cld_buffer':       cld_buffer,\n",
    "    'water_buffer':     water_buffer,\n",
    "    'cld_filt_thresh':  cld_filt_thresh,\n",
    "    'watermask':        watermask,\n",
    "    'harmonize_bnames': True,\n",
    "    'add_indices':      True,\n",
    "    'add_ratios':       True\n",
    "}\n",
    "\n",
    "# Run one batch task per stations\n",
    "tasks = []\n",
    "for station_id in tqdm(gdf_data.station_id.unique()):\n",
    "    fn = station_id\n",
    "    gdf_data_station = gdf_data.loc[gdf_data.station_id==station_id]\n",
    "    fc_station = ee.FeatureCollection(geemap.gdf_to_ee(\n",
    "        gdf_data_station, \n",
    "        date='dt_utc', date_format='YYYY-MM-dd HH:mm:ss'))\n",
    "    bounds = fc_station.geometry()\n",
    "    ic_oli = funcs_process.load_sr_imcoll(sensor='oli', bounds=bounds, **settings)\n",
    "    ic_etm = funcs_process.load_sr_imcoll(sensor='etm', bounds=bounds, **settings)\n",
    "    ic_msi = funcs_process.load_sr_imcoll(sensor='msi', bounds=bounds, **settings)\n",
    "    fc_matchups_oli = funcs_sampling.get_matchups(fc_station, ic_oli, max_diff=3)\n",
    "    fc_matchups_msi = funcs_sampling.get_matchups(fc_station, ic_msi, max_diff=3)\n",
    "    fc_matchups_etm = funcs_sampling.get_matchups(fc_station, ic_etm, max_diff=3)\n",
    "    fc_matchups = ee.FeatureCollection([fc_matchups_oli, fc_matchups_msi, fc_matchups_etm]).flatten() \\\n",
    "        .map(funcs_sampling.get_matchup_sample(buffer_dist=sampling_buffer))\n",
    "    # run GEE task\n",
    "    task = ee.batch.Export.table.toDrive(**{\n",
    "        'collection': fc_matchups, \n",
    "        'description': f'SR_{fn}',\n",
    "        'folder': output_folder})\n",
    "    task.start()\n",
    "    tasks.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = ic_oli.first()\n",
    "# Map = geemap.Map(center=(40, -100), zoom=4)\n",
    "# Map.centerObject(img)\n",
    "# Map.addLayer(img, {'bands':['is_cloud'], 'min':0, 'max':1, 'palette':['white', 'blue'], 'opacity':0.5}, 'Scene (is_cloud)')\n",
    "# Map.addLayer(img, {'bands':['is_water'], 'min':0, 'max':1, 'palette':['white', 'red'], 'opacity':0.5}, 'Scene (is_water)')\n",
    "# Map.addLayer(img.unmask(), {'bands':['red', 'green', 'blue'], 'min':0, 'max':0.3}, 'Scene')\n",
    "# Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405d2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_tasks_status(tasks):\n",
    "    \"\"\" Check the state of all provided ee.task objects and posts status updates. \"\"\"\n",
    "    colordict = {'white': '\\033[0m', 'red': '\\033[91m', 'orange': '\\033[93m', 'green': '\\033[92m'}\n",
    "    states = []\n",
    "    for task in tasks:\n",
    "        # get state and times\n",
    "        status = task.status()\n",
    "        state = status['state']\n",
    "        task_id = status['id']\n",
    "        time_start, time_update = status['creation_timestamp_ms'], status['update_timestamp_ms']\n",
    "        time_elapsed = timedelta(milliseconds=(time_update-time_start))\n",
    "        time_now = datetime.now()\n",
    "        # set output color\n",
    "        if state == 'COMPLETED':\n",
    "            color ='green'\n",
    "        elif (state == 'RUNNING') | (state == 'READY'):\n",
    "            color = 'orange'\n",
    "        elif (state == 'FAILED') | (state == 'CANCEL_REQUESTED') | (state == 'CANCELLED'):\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'white'\n",
    "        # print msg\n",
    "        status_msg = f\"[{str(time_now)[:19]}] Task {task_id}\" \\\n",
    "                     f\"({status.get('description', 'No description')}): {colordict[color]+state+colordict['white']}\"\n",
    "                     #f\" (runtime: {time_elapsed.seconds/60:0.1f}min)\"\n",
    "        print(status_msg)\n",
    "        states.append(state)\n",
    "    return states\n",
    "\n",
    "all_completed = False\n",
    "while not all_completed:\n",
    "    check_tasks_status(tasks)\n",
    "    states = [task.status()['state'] for task in tasks]\n",
    "    if all(state in ['COMPLETED', 'FAILED', 'CANCEL_REQUESTED'] for state in states):\n",
    "        all_completed = True\n",
    "        \n",
    "    else:\n",
    "        time.sleep(30)\n",
    "n_failed = sum([state in ['FAILED', 'CANCEL_REQUESTED'] for state in states])\n",
    "print(f\"All export tasks finished ({n_failed} tasks failed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c7ec3",
   "metadata": {},
   "source": [
    "## Import GEE outputs and parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267189c-994f-47cd-9eba-657459e9df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "path_output = Path(gdrive_folder).joinpath(output_folder)\n",
    "paths_csv = list(path_output.glob('*.csv'))\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "for path in paths_csv:\n",
    "    try:\n",
    "        df_temp = pd.read_csv(path)\n",
    "        df = pd.concat([df, df_temp])\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f'Note: {path.name} was empty. Skipping.')\n",
    "        continue # will skip the rest of the block and move to next file\n",
    "\n",
    "def parse_str_dict(string):\n",
    "    string = string.replace('=', '\":').replace(', ', ', \"').replace('null', 'None')[1:-1]\n",
    "    string = '{\"' + string +'}'\n",
    "    parsed_dict = dict(ast.literal_eval(string))\n",
    "    return parsed_dict\n",
    "\n",
    "df['match_values'] = df.match_values.apply(parse_str_dict)\n",
    "df_match_values = df['match_values'].apply(pd.Series)\n",
    "df = pd.concat([df.drop('match_values', axis=1), df_match_values], axis=1)\n",
    "df['match_td_days_abs'] = abs(df.match_td_days)\n",
    "# df.hvplot.scatter(x='add_ratio_rgb_mean', y='spm', c='match_td_days_abs', alpha=0.5, cmap='bwr')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620bed9",
   "metadata": {},
   "source": [
    "## Construct RF-Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dff189",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import functions_model as funcs_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import smogn\n",
    "\n",
    "# filter data\n",
    "df_data = df.copy().loc[~np.isnan(df.red_mean)]\n",
    "df_data = df_data.loc[abs(df_data.match_td_days)<=3]\n",
    "df_data = df_data.loc[(df_data.spm<=1000) & (df_data.spm>=0)]\n",
    "\n",
    "# sample equally for wet and dry season\n",
    "df_data['season'] = pd.to_datetime(df_data.dt_utc).dt.month.apply(lambda x: 'wet' if 6 <= x <= 11 else 'dry')\n",
    "n_wet = df_data.loc[df_data.season=='wet'].shape[0]  \n",
    "df_data = df_data.groupby('season', group_keys=False).apply(lambda x: x.sample(n_wet), include_groups=False)\n",
    "\n",
    "# filter features\n",
    "label = 'spm'\n",
    "smpl_method = 'mean' # sampling method\n",
    "#regex_expr = f'^(blue|red|green|nir|is_water|is_cloud|add_|{label})' # allow all starting with given str and target variable\n",
    "regex_expr = f'_{smpl_method}$|^{label}$' # allow only ending with _mean and target variable\n",
    "df_data = df_data.filter(regex=regex_expr)\n",
    "df_data.columns = df_data.columns.str.replace(f'_{smpl_method}$', '', regex=True)\n",
    "\n",
    "## Server-side model (GEE)\n",
    "# apply train-test-split and create server-side regressor\n",
    "# df_train, df_test, df_predicted, ee_rf_model = funcs_model.construct_ee_model(\n",
    "#     df_data, label=label,\n",
    "#     downsample=False,\n",
    "#     n_estimators=100,\n",
    "#     q=10, \n",
    "#     apply_smogn=True, \n",
    "#     smogn_settings={\n",
    "#         'rel_thres': 0.90,\n",
    "#         'k': 10,\n",
    "#         'rel_xtrm_type': 'high',\n",
    "#         'samp_method': 'extreme',\n",
    "#         'rel_method': 'auto',\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "## Local model for quick testing (sklearn)\n",
    "# apply train-test-split and create local regressor\n",
    "df_train, df_test, df_predicted, rf_model = funcs_model.construct_local_model(\n",
    "    df_data, label=label,\n",
    "    n_estimators=400,\n",
    "    q=10, \n",
    "    apply_pca=False, n_components=5,\n",
    "    log_transform=False,\n",
    "    apply_smogn=False, \n",
    "    smogn_settings={\n",
    "        'rel_thres': 0.90,\n",
    "        'k': 20,\n",
    "        'rel_xtrm_type': 'high',\n",
    "        'samp_method': 'extreme',\n",
    "        'rel_method': 'auto',\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Plot validation results\n",
    "predictions = df_predicted[f'{label}_predicted']\n",
    "test_labels = df_predicted[label]\n",
    "errors = abs(predictions - test_labels)\n",
    "mape = 100 * (errors / test_labels)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'mg/L.')\n",
    "print('Accuracy:', round(accuracy, 2), '%.')\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(test_labels, predictions))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(test_labels, predictions))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(test_labels, predictions)))\n",
    "mape = np.mean(np.abs((test_labels - predictions) / np.abs(test_labels)))\n",
    "print('Mean Absolute Percentage Error (MAPE):', round(mape * 100, 2))\n",
    "print('Accuracy:', round(100*(1 - mape), 2))\n",
    "\n",
    "# plot the results\n",
    "sorted_indices = np.argsort(test_labels)\n",
    "test_labels_sorted = test_labels[sorted_indices]\n",
    "predictions_sorted = predictions[sorted_indices]\n",
    "plt.plot(test_labels_sorted, predictions_sorted, '.', label=\"data\", alpha=0.25)\n",
    "plt.plot(test_labels_sorted, test_labels_sorted, '--', label=\"1:1\")\n",
    "plt.xlabel('$SPM_{insitu}$ (mg/L)')\n",
    "plt.ylabel('$SPM_{predicted}$ (mg/L)')\n",
    "plt.title(\"SPM-model Validation\")\n",
    "ax = plt.gca()\n",
    "ax.set_xlim([-10, 400])\n",
    "ax.set_ylim([-10, 400])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b20b17",
   "metadata": {},
   "source": [
    "## Export RF-Regressor model to GEE asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f6e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset_name = 'sosw_spm_rfregressor_sr_14052024'\n",
    "folder_name = r'projects/ee-soswater/assets/classifiers'\n",
    "asset_id = folder_name + '/' + asset_name\n",
    "try:\n",
    "    ee.data.createAsset({'type': 'Folder'}, folder_name)\n",
    "    print(f'Folder \"{folder_name}\" created successfully.')\n",
    "except Exception as e:\n",
    "    print(f'Error creating folder: {e}')\n",
    "\n",
    "task = ee.batch.Export.classifier.toAsset(\n",
    "    classifier=ee_rf_model,\n",
    "    description=asset_name,\n",
    "    assetId=asset_id\n",
    ")\n",
    "task.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85099b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_tasks_status([task])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
